{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break into sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/luba/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mytext = \"\"\"In the previous chapter, we saw examples of some common NLP\n",
    "applications that we might encounter in everyday life. If we were asked to\n",
    "build such an application, think about how we would approach doing so at our\n",
    "organization. We would normally walk through the requirements and break the\n",
    "problem down into several sub-problems, then try to develop a step-by-step\n",
    "procedure to solve them. Since language processing is involved, we would also\n",
    "list all the forms of text processing needed at each step. This step-by-step\n",
    "processing of text is known as pipeline. It is the series of steps involved in\n",
    "building any NLP model. These steps are common in every NLP project, so it\n",
    "makes sense to study them in this chapter. Understanding some common procedures\n",
    "in any NLP pipeline will enable us to get started on any NLP problem encountered\n",
    "in the workplace. Laying out and developing a text-processing pipeline is seen\n",
    "as a starting point for any NLP application development process. In this\n",
    "chapter, we will learn about the various steps involved and how they play\n",
    "important roles in solving the NLP problem and we’ll see a few guidelines\n",
    "about when and how to use which step. In later chapters, we’ll discuss\n",
    "specific pipelines for various NLP tasks (e.g., Chapters 4–7).\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will break our text into sentences\n",
    "my_sentences = nltk.sent_tokenize(mytext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In the previous chapter, we saw examples of some common NLP\\napplications that we might encounter in everyday life.',\n",
       " 'If we were asked to\\nbuild such an application, think about how we would approach doing so at our\\norganization.',\n",
       " 'We would normally walk through the requirements and break the\\nproblem down into several sub-problems, then try to develop a step-by-step\\nprocedure to solve them.',\n",
       " 'Since language processing is involved, we would also\\nlist all the forms of text processing needed at each step.',\n",
       " 'This step-by-step\\nprocessing of text is known as pipeline.',\n",
       " 'It is the series of steps involved in\\nbuilding any NLP model.',\n",
       " 'These steps are common in every NLP project, so it\\nmakes sense to study them in this chapter.',\n",
       " 'Understanding some common procedures\\nin any NLP pipeline will enable us to get started on any NLP problem encountered\\nin the workplace.',\n",
       " 'Laying out and developing a text-processing pipeline is seen\\nas a starting point for any NLP application development process.',\n",
       " 'In this\\nchapter, we will learn about the various steps involved and how they play\\nimportant roles in solving the NLP problem and we’ll see a few guidelines\\nabout when and how to use which step.',\n",
       " 'In later chapters, we’ll discuss\\nspecific pipelines for various NLP tasks (e.g., Chapters 4–7).']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break into tokens (tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['If',\n",
       " 'we',\n",
       " 'were',\n",
       " 'asked',\n",
       " 'to',\n",
       " 'build',\n",
       " 'such',\n",
       " 'an',\n",
       " 'application',\n",
       " ',',\n",
       " 'think',\n",
       " 'about',\n",
       " 'how',\n",
       " 'we',\n",
       " 'would',\n",
       " 'approach',\n",
       " 'doing',\n",
       " 'so',\n",
       " 'at',\n",
       " 'our',\n",
       " 'organization',\n",
       " '.']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(my_sentences[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing stop words, digits, punctuation and lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(texts):\n",
    "    mystopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def remove_stop_digits(tokens):\n",
    "        return [\n",
    "            token.lower()\n",
    "            for token in tokens\n",
    "            if token not in mystopwords\n",
    "            and not token.isdigit()\n",
    "            and token not in punctuation\n",
    "        ]\n",
    "\n",
    "    return [remove_stop_digits(nltk.word_tokenize(text)) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_test = \"Oh sheat, look at this\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['o'],\n",
       " ['h'],\n",
       " [],\n",
       " [],\n",
       " ['h'],\n",
       " ['e'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['l'],\n",
       " [],\n",
       " [],\n",
       " ['k'],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " [],\n",
       " ['h'],\n",
       " [],\n",
       " []]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_corpus(text_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "card revolut\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "word1, word2 = \"cards\", \"revolutions\"\n",
    "print(stemmer.stem(word1), stemmer.stem(word2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at this lemmatizer based on WordNet from NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print(lemmatizer.lemmatize(\"better\", pos=\"a\"))  # a is for adjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "better well\n"
     ]
    }
   ],
   "source": [
    "# using spacy\n",
    "import spacy\n",
    "sp = spacy.load(\"en_core_web_sm\")\n",
    "token = sp(u'better')\n",
    "for word in token:\n",
    "    print(word.text, word.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Charles Charles PROPN Xxxxx True False\n",
      "Spencer Spencer PROPN Xxxxx True False\n",
      "Chaplin Chaplin PROPN Xxxxx True False\n",
      "was be AUX xxx True True\n",
      "born bear VERB xxxx True False\n",
      "on on ADP xx True True\n",
      "16 16 NUM dd False False\n",
      "april april PROPN xxxx True False\n",
      "1889 1889 NUM dddd False False\n",
      "toHannah toHannah PROPN xxXxxxx True False\n",
      "Chaplin Chaplin PROPN Xxxxx True False\n",
      "born bear VERB xxxx True False\n",
      "Hannah Hannah PROPN Xxxxx True False\n",
      "Harriet Harriet PROPN Xxxxx True False\n",
      "Pedlingham Pedlingham PROPN Xxxxx True False\n",
      "Hill Hill PROPN Xxxx True False\n",
      "and and CCONJ xxx True True\n",
      "Charles Charles PROPN Xxxxx True False\n",
      "Chaplin Chaplin PROPN Xxxxx True False\n",
      "Sr Sr PROPN Xx True False\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\n",
    "    u\"Charles Spencer Chaplin was born on 16 april 1889 toHannah Chaplin born Hannah Harriet Pedlingham Hill and Charles Chaplin Sr\"\n",
    ")\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.shape_, token.is_alpha, token.is_stop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "68238cbedf6ae1d25174a29dd4170dba295aa8ea613e27a77673212528c59789"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('NLPPersonalStudy-APklfgYW': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
